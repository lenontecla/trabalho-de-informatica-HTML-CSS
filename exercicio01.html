<html>
	<head>
		<title>História da Computação</title>
		<link rel="stylesheet" href="exercicio01.css">
	</head>
	<body>
		<div class="header">
			<a class="top-title">Trabalho de informática</a>
			<div class="header-right">
			</div>
		</div>
		
		<br><br><br><br>
	
		<h1>História da computação</h1>
		<em>Uma perspectiva histórica importante</em>
		<br>
		<br>
		<strong>IFRS - Alvorada</strong>
		<p>
			O desenvolvimento da tecnologia da computação foi a união de várias áreas do conhecimento humano, dentre as
			quais: a matemática, a eletrônica digital, a lógica de programação, entre outras.
		</p>

		<h2>Computação</h2>
		<p>
			A capacidade do ser humano em calcular quantidades nos mais variados modos foi um dos fatores que
			possibilitaram o desenvolvimento da matemática e da lógica. Nos primórdios da matemática e da álgebra,
			utilizavam-se os dedos das mãos para efetuar cálculos.
		</p>
		<p>
			A primeira ferramenta conhecida para a computação foi o <strong>ábaco</strong>, cuja invenção é atribuída a habitantes da
			Mesopotâmia, em torno de 2400 a.C. O uso original era desenhar linhas na areia com rochas. Versões mais
			modernas do ábaco ainda são usadas como instrumento de cálculo.
		</p>
		<p>
			O ábaco dos romanos consistia de bolinhas de mármore que deslizavam numa placa de bronze cheia de sulcos.
			Também surgiram alguns termos matemáticos: em latim "Calx" significa mármore, assim "Calculos" era uma
			bolinha do ábaco, e fazer cálculos aritméticos era "Calculare".
		</p>
		<p>
			No século V a.C., na antiga Índia, o gramático <em>Panini</em> formulou a gramática de Sânscrito usando 3959 regras
			conhecidas como <em>Ashtadhyayi</em>, de forma bastante sistemática e técnica. Panini usou meta-regras,
			transformações e recursividade com tamanha sofisticação que sua gramática possuía o poder computacional
			teórico tal qual a Máquina de Turing.
		</p>
		<p>
			Entre 200 a.C. e 400, os indianos também inventaram o <em>logaritmo</em>, e a partir do século XIII tabelas
			logarítmicas eram produzidas por matemáticos islâmicos. Quando John Napier descobriu os logaritmos para uso
			computacional no século XVI, seguiu-se um período de considerável progresso na construção de ferramentas de
			cálculo.
		</p>
		<p>
			<em>John Napier</em> (1550-1617), escocês inventor dos logaritmos, também inventou os <strong>ossos de Napier</strong>, que eram
			tabelas de multiplicação gravadas em bastão, o que evitava a memorização da tabuada.
		</p>
		<p>
			A primeira máquina de verdade foi construída por <em>Wilhelm Schickard</em> (1592-1635), sendo capaz de somar,
			subtrair, multiplicar e dividir. Essa máquina foi perdida durante a guerra dos trinta anos, sendo que
			recentemente foi encontrada alguma documentação sobre ela. Durante muitos anos nada se soube sobre essa
			máquina, por isso, atribuía-se a <strong>Blaise Pascal</strong> (1623-1662) a construção da primeira máquina calculadora,
			que fazia apenas somas e subtrações.
		</p>
		<p>
			Pascal, que aos 18 anos trabalhava com seu pai em um escritório de coleta de impostos na cidade de Rouen,
			desenvolveu a máquina para auxiliar o seu trabalho de contabilidade. A calculadora usava engrenagens que a
			faziam funcionar de maneira similar a um odômetro.
		</p>
		<p>
			Pascal recebeu uma patente do rei da França para que lançasse sua máquina no comércio. A comercialização de
			suas calculadoras não foi satisfatória devido a seu funcionamento pouco confiável, apesar de Pascal ter
			construído cerca de 50 versões.
		</p>
		<p>
			A máquina Pascal foi criada com objetivo de ajudar seu pai a computar os impostos em Rouen, França. O
			projeto de Pascal foi bastante aprimorado pelo matemático alemão <strong>Gottfried Wilhelm Leibniz</strong> (1646-1726),
			que também inventou o cálculo, o qual sonhou que, um dia no futuro, todo o raciocínio pudesse ser
			substituído pelo girar de uma simples alavanca.
		</p>
		<p>
			Em 1671, o filósofo e matemático alemão de Leipzig, Gottfried Wilhelm Leibniz introduziu o conceito de
			realizar multiplicações e divisões através de adições e subtrações sucessivas. Em 1694, a máquina foi
			construída, no entanto, sua operação apresentava muita dificuldade e sujeita a erros.
		</p>
		<p>
			Em 1820, o francês natural de Paris, <em>Charles Xavier Thomas</em>, conhecido como <em>Thomas de Colmar</em>, projetou e
			construiu uma máquina capaz de efetuar as 4 operações aritméticas básicas: a <strong>Arithmomet</strong>. Esta foi a
			primeira calculadora realmente comercializada com sucesso. Ela fazia multiplicações com o mesmo princípio
			da calculadora de Leibnitz e efetuava as divisões com a assistência do usuário.
		</p>
		<p>
			Todas essas máquinas, porém, estavam longe de ser considerado um computador, pois não eram programáveis.
			Isto quer dizer que a entrada era feita apenas de números, mas não de instruções a respeito do que fazer
			com os números.
		</p>
		<h2>Os Algoritmos</h2>
		<p>
			No século VII, o matemático indiano <em>Brahmagupta</em> explicou pela primeira vez o sistema de numeração
			hindu-arábico e o uso do 0. Aproximadamente em 825, o matemático persa <em>Al-Khwarizmi</em> escreveu o livro
			<em>Calculando com numerais hindus</em>, responsável pela difusão do sistema de numeração hindu-arábico no Oriente
			Médio, e posteriormente na Europa. Por volta do século XII houve uma tradução do mesmo livro para o latim:
			<em>Algoritmi de número Indorum</em>. Tais livros apresentaram novos conceitos para definir sequências de passos
			para completar tarefas, como aplicações de aritmética e álgebra. Por derivação do nome, atualmente usa-se o
			termo <strong>algoritmo</strong>.
		</p>
		<h2>A Revolução Industrial</h2>
		<p>
			Em 1801, na França, durante a Revolução Industrial, <strong>Joseph Marie Jacquard</strong>, mecânico francês, (1752-1834)
			inventou um tear mecânico controlado por grandes cartões perfurados. Sua máquina era capaz de produzir
			tecidos com desenhos bonitos e intrincados. Foi tamanho o sucesso que Jacquard foi quase morto quando
			levou o tear para Lyon, pois as pessoas tinham medo de perder o emprego. Em sete anos, já havia 11 mil
			teares desse tipo operando na França.
		</p>
		<h2>Babbage e Ada</h2>
		<p>
			A ideia de Jacquard atravessou o Canal da Mancha, onde inspirou <strong>Charles Babbage</strong> (1792-1871), um professor
			de matemática de Cambridge, a desenvolver uma máquina de "tecer números", uma máquina de calcular onde a
			forma de calcular pudesse ser controlada por cartões.
		</p>
		<p>
			Foi com Charles Babbage que o computador moderno começou a ganhar forma, através de seu trabalho no
			<strong>engenho analítico</strong>. O equipamento, apesar de nunca ter sido construído com sucesso, possuía todas as
			funcionalidades do computador moderno. Foi descrito originalmente em 1837, mais de um século antes que
			qualquer equipamento do gênero tivesse sido construído com sucesso. O grande diferencial do sistema de
			Babbage era o fato que seu dispositivo foi projetado para ser programável, item imprescindível para
			qualquer computador moderno.
		</p>
		<p>
			Tudo começou com a tentativa de desenvolver uma máquina capaz de calcular polinômios por meio de
			diferenças, o <strong>calculador diferencial</strong>. Enquanto projetava seu calculador diferencial, a ideia de Jacquard
			fez com que Babbage imaginasse uma nova e mais complexa máquina, o calculador analítico, extremamente
			semelhante ao computador atual.
		</p>
		<p>
			O projeto, totalmente mecânico, era composto de uma memória, um engenho central, engrenagens e alavancas
			usadas para a transferência de dados da memória para o engenho central e dispositivos para entrada e saída
			de dados. O calculador utilizaria cartões perfurados e seria automático.
		</p>
		<p>
			Sua parte principal seria um conjunto de rodas dentadas, o moinho, formando uma máquina de somar com
			precisão de cinquenta dígitos. As instruções seriam lidas de cartões perfurados. Os cartões seriam lidos
			em um dispositivo de entrada e armazenados, para futuras referências, em um banco de mil registradores.
			Cada um dos registradores seria capaz de armazenar um número de cinquenta dígitos, que poderiam ser
			colocados lá por meio de cartões a partir do resultado de um dos cálculos do moinho.
		</p>
		<p>
			Por algum tempo, o governo britânico financiou Babbage para construir a sua invenção.
		</p>
		<p>
			Além disso tudo, Babbage imaginou a primeira máquina de impressão, que imprimiria os resultados dos
			cálculos, contidos nos registradores. Babbage conseguiu, durante algum tempo, fundos para sua pesquisa,
			porém não conseguiu completar sua máquina no tempo prometido e não recebeu mais dinheiro. Hoje, partes de
			sua máquina podem ser vistas no Museu Britânico, que também construiu uma versão completa, utilizando as
			técnicas disponíveis na época.
		</p>
		<p>
			Durante sua colaboração, a matemática <strong>Ada Lovelace</strong> publicou os primeiros programas de computador em uma
			série de notas para o engenho analítico. Por isso, Lovelace é popularmente considerada como a <em>primeira
			programadora</em>. Ela se tornou a pioneira da lógica de programação,
			escrevendo séries de instruções para o calculador analítico. Ada inventou os conceitos de <em>subrotina</em>, uma
			seqüência de instruções que pode ser usada várias vezes, <em>loop</em>, uma instrução que permite a repetição de
			uma sequência de cartões, e do <em>salto condicional</em>, que permite saltar algum cartão caso uma condição seja
			satisfeita.
		</p>
		<p>
			Babbage teve muitas dificuldades com a tecnologia da época, que era inadequada para se construir
			componentes mecânicos com a precisão necessária. Com a suspensão do financiamento por parte do governo
			britânico, Babbage e Ada utilizaram a fortuna da família Byron até a falência, sem que pudessem concluir
			o projeto, e assim o calculador analítico nunca foi construído.
		</p>
		<p>
			Ada Lovelace e Charles Babbage estavam avançados demais para o seu tempo, tanto que até a década de 1940,
			nada se inventou parecido com seu computador analítico. Até essa época foram construídas muitas máquinas
			mecânicas de somar destinadas a controlar negócios (principalmente caixas registradoras) e algumas
			máquinas inspiradas na calculadora diferencial de Babbage, para realizar cálculos de engenharia (que não
			alcançaram grande sucesso).
		</p>
		<h2>A Lógica Binária</h2>
		<p>
			Por volta do século III a.C., o matemático indiano <em>Pingala</em> inventou o <em>sistema de numeração binário</em>. Ainda
			usado atualmente no processamento de todos computadores modernos, o sistema estabelece que sequências
			específicas de uns e zeros podem representar qualquer número, letra ou imagem.
		</p>
		<p>
			Em 1703 <em>Gottfried Leibniz</em> desenvolveu a lógica em um sentido formal e matemático, utilizando o sistema
			binário. Em seu sistema, uns e zeros também representam conceitos como verdadeiro e falso, ligado e
			desligado, válido e inválido. Levou mais de um século para que George Boole publicasse a álgebra booleana
			(em 1854), com um sistema completo que permitia a construção de modelos matemáticos para o processamento
			computacional. Em 1801 apareceu o tear controlado por cartão perfurado, invenção de Joseph Marie Jacquard,
			no qual buracos indicavam os uns, e áreas não furadas indicavam os zeros. O sistema está longe de ser um
			computador, mas ilustrou que as máquinas poderiam ser controladas pelo sistema binário.
		</p>
		<p>
			As máquinas do início do século XIX utilizavam base decimal (0 a 9), mas foram encontradas dificuldades
			em implementar um dígito decimal em componentes eletrônicos, pois qualquer variação provocada por um
			ruído causaria erros de cálculo consideráveis.
		</p>
		<p>
			O matemático inglês <strong>George Boole</strong> (1815-1864) publicou em 1854 os princípios da <strong>lógica booleana</strong>, onde as
			variáveis assumem apenas valores 0 e 1 (verdadeiro e falso), que passou a ser utilizada a partir do
			início do século XX.
		</p>
		<h2>Shannon e a Teoria da Informação</h2>
		<p>
			Até a década de 1930, engenheiros eletricistas podiam construir circuitos eletrônicos para resolver
			problemas lógicos e matemáticos, mas a maioria o fazia sem qualquer processo, de forma particular, sem
			rigor teórico para tal. Isso mudou com a tese de mestrado de <strong>Claude E. Shannon</strong> de 1937, <em>A Symbolic
			Analysis of Relay and Switching Circuits</em>. Enquanto tomava aulas de Filosofia, Shannon foi exposto ao
			trabalho de George Boole, e percebeu que tal conceito poderia ser aplicado em conjuntos eletro-mecânicos
			para resolver problemas de lógica. Tal ideia, que utiliza propriedades de circuitos eletrônicos para a lógica,
			é o conceito básico de todos os computadores digitais. Shannon desenvolveu a teoria da informação no artigo de
			1948, <em>A Mathematical Theory of Communication</em>, cujo conteúdo serve como fundamento para áreas de estudo como
			compressão de dados e criptografia.
		</p>
		<h2>Hollerith e sua máquina de perfurar cartões</h2>
		<p>
			Por volta de 1890, o <strong>Dr. Herman Hollerith</strong> (1860-1929) foi o responsável por uma grande mudança na maneira
			de se processar os dados dos censos da época. Ele inventou uma máquina capaz de processar dados baseada
			na separação de cartões perfurados (pelos seus furos). A máquina de Hollerith foi utilizada para auxiliar
			o censo de 1890, reduzindo o tempo de processamento de dados de sete anos, do censo anterior, para apenas
			dois anos e meio. Ela foi também pioneira ao utilizar a eletricidade na separação, contagem e tabulação
			dos cartões.
		</p>
		<p>
			As informações sobre os indivíduos eram armazenadas por meio de perfurações em locais específicos do
			cartão. Nas máquinas de tabular, um pino passava pelo furo e chegava a uma jarra de mercúrio, fechando um
			circuito elétrico e causando um incremento de 1 em um contador mecânico.
		</p>
		<p>
			Mais tarde, Hollerith fundou uma companhia para produzir máquinas de tabulação. Anos depois, em 1924,
			essa companhia veio a se chamar <strong>International Business Machines</strong>, ou <strong>IBM</strong>, como é hoje conhecida.
		</p>
		<h2>O primeiro computador</h2>
		<p>
			O primeiro computador eletro-mecânico foi construído por <strong>Konrad Zuse</strong> (1910-1995). Em 1936, esse
			engenheiro alemão construiu, a partir de relés que executavam os cálculos e dados lidos em fitas
			perfuradas, o <strong>Z1</strong>.
		</p>
		<p>
			Zuse tentou vender o computador ao governo alemão, que desprezou a oferta, já que não poderia auxiliar no
			esforço de guerra. Os projetos de Zuse ficariam parados durante a guerra, dando a chance aos americanos
			de passar à frente na corrida tecnológica.
		</p>
		<h2>A guerra e os computadores</h2>
		<p>
			Foi na Segunda Guerra Mundial que realmente nasceram os computadores atuais. A Marinha americana, em
			conjunto com a Universidade de Harvard, desenvolveu o computador <strong>Harvard Mark I</strong>, projetado pelo professor
			<strong>Howard Aiken</strong>, com base no calculador analítico de Babbage. O Mark I ocupava 120m³ aproximadamente,
			conseguindo multiplicar dois números de dez dígitos em três segundos.
		</p>
		<p>
			Em segredo, o exército norte-americano também desenvolvia seu computador. Esse usava apenas válvulas e
			tinha por objetivo calcular as trajetórias de mísseis com maior precisão.
		</p>
		<p>
			Simultaneamente, e em segredo, o Exército Americano desenvolvia um projeto semelhante, chefiado pelos
			engenheiros <em>J. Presper Eckert</em> e <em>John Mauchy</em>, cujo resultado foi o primeiro computador a válvulas, o
			<strong>Eletronic Numeric Integrator And Calculator (ENIAC)</strong>, capaz de fazer quinhentas multiplicações por
			segundo. Tendo sido projetado para calcular trajetórias balísticas, o ENIAC foi mantido em segredo pelo
			governo americano até o final da guerra, quando foi anunciado ao mundo.
		</p>
		<p>
			Os custos para a manutenção e conservação do ENIAC eram proibitivos, pois dezenas a centenas de válvulas
			queimavam a cada hora e o calor gerado por elas necessitava ser controlado por um complexo sistema de
			refrigeração, além dos gastos elevadíssimos de energia elétrica.
		</p>
		<p>
			No ENIAC, o programa era feito rearranjando a fiação em um painel. Nesse ponto <strong>John von Neumann</strong> propôs a
			ideia que transformou os calculadores eletrônicos em "cérebros eletrônicos": modelar a arquitetura do
			computador segundo o sistema nervoso central. Para isso, eles teriam que ter três características:
		</p>
		<p class="itemlista">
			1. Codificar as instruções de uma forma possível de ser armazenada na memória do computador. Von Neumann
			sugeriu que fossem usados uns e zeros.
		</p>
		<p class="itemlista">
			2. Armazenar as instruções na memória, bem como toda e qualquer informação necessária a
			execução da tarefa.
		</p>
		<p class="itemlista">
			3. Quando processar o programa, buscar as instruções diretamente na memória, ao invés de lerem um novo
			cartão perfurado a cada passo.
		</p>
		<p>
			Este é o conceito de programa armazenado, cujas principais vantagens são: rapidez, versatilidade e
			automodificação. Assim, o computador programável que conhecemos hoje, onde o programa e os dados estão
			armazenados na memória ficou conhecido como Arquitetura de von Neumann.
		</p>
		<p>
			Para divulgar essa ideia, von Neumann publicou sozinho um artigo. Eckert e Mauchy não ficaram muito
			contentes com isso, pois teriam discutido muitas vezes com ele. O projeto ENIAC acabou se dissolvendo em
			uma chuva de processos, mas já estava criado o computador moderno.
		</p>
		<h2>O nascimento da Ciência da Computação</h2>
		<p>
			Antes da década de 1920, o computador era um termo associado a pessoas que realizavam cálculos,
			geralmente liderados por físicos em sua maioria homens. Milhares de computadores, eram empregados em
			projetos no comércio, governo e sítios de pesquisa. Após a década de 1920, a expressão máquina
			computacional começou a ser usada para referir-se a qualquer máquina que realize o trabalho de um
			profissional computador, especialmente aquelas de acordo com os métodos da Tese de Church-Turing.
		</p>
		<p>
			O termo máquina computacional acabou perdendo espaço para o termo reduzido computador no final da década
			de 1940, com as máquinas digitais cada vez mais difundidas.
		</p>
		<h2>O Trabalho Teórico</h2>
		<p>
			Os fundamentos matemáticos da ciência da computação moderna começaram a serem definidos por <em>Kurt Gödel</em>
			com seu <em>teorema da incompletude</em> (1931). Essa teoria mostra que existem limites no que pode ser provado ou
			desaprovado em um sistema formal; isso levou a trabalhos posteriores por Gödel e outros teóricos para
			definir e descrever tais sistemas formais, incluindo conceitos como <em>recursividade</em> e <em>cálculo lambda</em>.
		</p>
		<p>
			Em 1936, <strong>Alan Turing</strong> e <strong>Alonzo Church</strong> independentemente, e também juntos, introduziram a formalização de
			um algoritmo, definindo os limites do que pode ser computado, e um modelo puramente mecânico para a
			computação. Tais tópicos são abordados no que atualmente chama-se <strong>Tese de Church-Turing</strong>, uma hipótese
			sobre a natureza de dispositivos mecânicos de cálculo. Essa tese define que qualquer cálculo possível
			pode ser realizado por um algoritmo sendo executado em um computador, desde que haja tempo e
			armazenamento suficiente para tal.
		</p>
		<p>
			Turing também incluiu na tese uma descrição da <strong>Máquina de Turing</strong>, que possui uma fita de tamanho infinito
			e um cabeçote para leitura e escrita que move-se pela fita. Devido ao seu caráter infinito, tal máquina
			não pode ser construída, mas tal modelo pode simular a computação de qualquer algoritmo executado em um
			computador moderno. Turing é bastante importante para a ciência da computação, tanto que seu nome é usado
			para o <em>Turing Award</em> e o <em>teste de Turing</em>. Ele contribuiu para as quebras de código da Grã-Bretanha na
			Segunda Guerra Mundial, e continuou a projetar computadores e programas de computador pela década de 1940.
			Cometeu suicídio em 1954.
		</p>
		<h2>Primeiros computadores pessoais</h2>
		<p>
			Até o final dos anos 1970, reinavam absolutos os <strong>mainframes</strong>, computadores enormes, trancados em salas
			refrigeradas e operados apenas por alguns poucos privilegiados. Apenas grandes empresas e bancos podiam
			investir alguns milhões de dólares para tornar mais eficientes alguns processos internos e o fluxo de
			informações. A maioria dos escritórios funcionava mais ou menos da mesma maneira que no começo do século. Arquivos de metal, máquinas de escrever, papel carbono e memorandos faziam parte do dia-a-dia.
		</p>
		<p>
			Segundo o <em>Computer History Museum</em>, o primeiro "computador pessoal" foi o <strong>Kenbak-1</strong>, lançado em 1971. Tinha
			256 bytes de memória e foi anunciado na revista <em>Scientific American</em> por US$ 750; todavia, não possuía CPU
			e era, como outros sistemas desta época, projetado para uso educativo (ou seja, demonstrar como um
			"computador de verdade" funcionava). Em 1975, surge o <strong>Altair 8800</strong>, um computador pessoal baseado na CPU
			<strong>Intel 8080</strong>. Vendido originalmente como um kit de montar através da revista norte-americana <em>Popular
			Electronics</em>, os projetistas pretendiam vender apenas algumas centenas de unidades, tendo ficado surpresos
			quando venderam 10 vezes mais que o previsto para o primeiro mês. Custava cerca de 400 doláres e se
			comunicava com o usuário através de luzes que piscavam. Entre os primeiros usuários estavam o calouro da
			Universidade de Harvard, <strong>Bill Gates</strong>, e o jovem programador, <strong>Paul Allen</strong>, que juntos desenvolveram uma
			versão da linguagem <em>Basic</em> para o Altair. Pouco tempo depois, a dupla resolveu mudar o rumo de suas
			carreiras e criar uma empresa chamada <strong>Microsoft</strong>.
		</p>
		<p>
			Nos anos seguintes, surgiram dezenas de novos computadores pessoais como o <em>Radio Shack TRS-80</em> (O TRS-80
			foi comercializado com bastante sucesso no Brasil pela <em>Prológica</em> com os nomes de <em>CP-300</em> e <em>CP-500</em>),
			<em>Commodore 64</em>, <em>Atari 400</em> e outros com sucesso moderado.
		</p>
		<h2>A Apple e a popularização</h2>
		<p>
			Em 1976, outra dupla de jovens, <strong>Steve Jobs</strong> e <strong>Steve Wozniak</strong>, iniciou outra empresa que mudaria o rumo da
			informática: a <strong>Apple</strong>.
		</p>
		<p>
			Jobs e Wozniak abandonaram a Universidade de Berkeley para poderem se dedicar ao projeto de computador
			pessoal criado por Wozniak, o <em>Apple I</em>. Como Wozniak trabalhava para a HP, o seu projeto precisava ser
			apresentado para a empresa que recusou de imediato a idéia. Isso abriu o caminho para a criação da Apple,
			empresa fundada pelos dois que comercializaria os computadores. Montados na garagem de Jobs, os 200
			primeiros computadores foram vendidos nas lojas da vizinhança a US$ 500 cada. Interessado no projeto,
			Mike Makula (na época vice-presidente de marketing da Intel), resolveu investir US$ 250 mil na Apple.
		</p>
		<p>
			Alguns meses depois, já em 1977, foi lançado o primeiro microcomputador como conhecemos hoje, o <strong>Apple II</strong>.
			O equipamento já vinha montado, com teclado integrado e era capaz de gerar gráficos coloridos. Parte da
			linguagem de programação do Apple II havia sido feita pela Microsoft, uma variação do BASIC para o
			Apple II. As vendas chegaram a US$ 2,5 milhões no primeiro ano de comercialização e, com o seu rapido
			crescimento de vendas, a Apple tornou-se uma empresa pública (ou seja, com ações que podem ser adquiridas
			por qualquer um na bolsa de valores) e ela construiu a sua sede principal - Infinite Loop - em Cupertino,
			Califórnia.
		</p>
		<p>
			Com o sucesso do Apple II, vieram o <em>Visicalc</em> (a primeira planilha eletrônica), processadores de texto e
			programas de banco de dados. Os micros já podiam substituir os fluxos de caixa feitos com cadernos e
			calculadoras, máquinas de escrever e os arquivos de metal usados para guardar milhares de documentos. Os
			computadores domésticos deixaram então de ser apenas um hobby de adolescentes para se tornarem
			ferramentas indispensáveis para muitas pessoas.
		</p>
		<p>
			Entretanto, até o começo dos anos 1980, muitos executivos ainda encaravam os computadores pessoais como
			brinquedos. Além das mudanças de hábitos necessárias para aproveitar a nova tecnologia, os mais
			conservadores tinham medo de comprar produtos de empresas dirigidas por um rapaz de 26 anos que há menos
			de 5 trabalhava na garagem dos pais.
		</p>
		<h2>Os computadores pessoais para empresas</h2>
		<p>
			Em 1980, a IBM estava convencida de que precisava entrar no mercado da microinformática e o uso
			profissional dos micros só deslanchou quando ela entrou nesse mercado. A empresa dominava (e domina até
			hoje) o mercado de computadores de grande porte e, desde a primeira metade do século XX, máquinas de
			escrever com sua marca estavam presentes nos escritórios de todo mundo. Como não estava acostumada à
			agilidade do novo mercado, criado e dominado por jovens dinâmicos e entusiasmados, a gigantesca
			corporação decidiu que o PC não podia ser criado na mesma velocidade na qual ela estava acostumada a
			desenvolver novos produtos.
		</p>
		<p>
			Por isso, a empresa criou uma força tarefa especial para desenvolver o novo produto. Assim, um grupo de
			12 engenheiros liderados por <em>William C. Lowe</em> foi instalado em um laboratório em Boca Raton, na Flórida,
			longe dos principais centros de desenvolvimento da corporação que, até hoje, ficam na Califórnia e em
			Nova Iorque. O resultado desse trabalho foi o <strong>IBM-PC</strong>, que tinha um preço de tabela de US$ 2.820, bem mais
			caro que os concorrentes, mas foi um sucesso imediato. Em 4 meses foram vendidas 35 mil unidades, 5 vezes
			mais do que o esperado. Como observou o jornalista Robert Cringley: "ninguém nunca tinha sido despedido
			por comprar produtos IBM". Os micros deixaram definitivamente de ser um brinquedo.
		</p>
		<h2>A Parceria IBM - Microsoft</h2>
		<p>
			Como todo computador, o IBM PC precisava de um Sistema Operacional para poder ser utilizado. Durante o
			processo de desenvolvimento do IBM PC, houve uma tentativa sem sucesso de contratar a <em>Digital Research</em>,
			uma empresa experiente na criação de Sistemas Operacionais, para o desenvolvimento do Sistema Operacional
			da IBM.
		</p>
		<p>
			Sem outra alternativa, a IBM recorreu a Microsoft que ofereceu um Sistema Operacional para a IBM, mas na
			verdade eles não tinham nada pronto. Ao assinar o contrato de licenciamento do <strong>DOS (Disk Operating System
			- Sistema Operacional de Disco)</strong> para a IBM, Bill Gates e Paul Allen foram atrás da <em>Seatlle Computer</em>, uma
			pequena empresa que desenvolvia o <em>Sistema Operacional QDOS</em> e que o vendeu para a Microsoft por US$ 50.000
			sem imaginar o fim que esse sistema teria.
		</p>
		<p>
			A Microsoft então adaptou-o e criou o <strong>PC-DOS</strong>. O contrato com a IBM previa uma royalty (de 10 a 50 dólares
			por cada máquina vendida) e um pequeno pagamento inicial. Mas o sistema continuava sobre propriedade da
			Microsoft, assim como a possibilidade de distribuir versões modificadas <strong>(MS-DOS)</strong>.
		</p>
		<p>
			Esse contrato é, sem dúvida alguma, um dos mais importantes do século XX, pois, através desse contrato, a
			Microsoft deixou de ser uma microempresa de software para se tornar a empresa mais poderosa no ramo da
			informática e tornar Bill Gates um dos homens mais ricos do mundo atual.
		</p>
		<h2>A aposta da Apple para continuar no topo</h2>
		<p>
			Em dezembro de 1979, a Apple Computer era a empresa de maior sucesso da microinformática. O carro chefe
			da empresa, o Apple II+ já estava presente em escolas e residências da elite americana. Entretanto, as
			máquinas ainda eram difíceis de usar. Para operar um microcomputador, era preciso conhecer a "linguagem"
			do sistema operacional e a sintaxe correta para aplicá-la. Todas as interações do usuário com a máquina
			eram feitas através da digitação de comandos. Uma letra errada e a operação não era realizada, exigindo a
			digitação do comando correto. Assim, antes de aproveitar os benefícios da informática, era indispensável
			aprender todos os comandos de controle do computador. O computador da Apple estava com quase 2 anos de
			existência e já começava a ficar velho. A empresa precisava criar algo novo para continuar competindo.
		</p>
		<p>
			A <em>Xerox</em>, empresa que dominava o mercado de copiadoras, acreditava que o seu negócio poderia perder
			rentabilidade com a redução do fluxo de documentos em papel, por causa do uso de documentos em formato
			eletrônico. Foi criado então, em 1970, o <em>Palo Alto Research Center (PARC)</em> com o intuito de inventar o
			futuro. Nessa época o PARC desenvolvia muitas novidades como as redes locais e impressoras laser, mas a
			pesquisa mais importante era a interface gráfica e o mouse. Após grandes desastres na tentativa de
			comercializar computadores do PARC (o computador do PARC saia por US$ 17 mil enquanto o da IBM custava
			apenas US$ 2,8 mil), a Xerox desistiu do projeto.
		</p>
		<p>
			Steve Jobs também desenvolvia nos laboratórios da Apple a interface gráfica. Buscando saber detalhes de
			como ela ficaria depois de pronta, trocou opções de compra de ações da Apple por uma visita detalhada de
			três dias ao PARC. O primeiro produto lançado pela Apple usando os conceitos criados pela Xerox foi o
			<strong>Lisa</strong>. Apesar de moderno, não chegou a ser produzido em grande quantidade, pois o mercado não estava
			preparado para pagar quase US$ 10 mil apenas pela facilidade de uso.
		</p>
		<p>
			Em 1979 <em>Jef Raskin</em>, um especialista em interfaces homem-máquina, imaginou um computador fácil de utilizar
			e barato para o grande público. Ele então lançou as bases do projeto <strong>Macintosh</strong>. O projeto inovador do
			Macintosh atraiu a atenção de Steve Jobs, que saiu do projeto Lisa com sua equipe para se concentrar no
			projeto Macintosh. Em janeiro de 1981, ele tomou a direção do projeto, forçando Jef Raskin a deixar o
			mesmo.
		</p>
		<p>
			Em 24 de janeiro de 1984 surgiu o <strong>Macintosh</strong>, o primeiro computador de sucesso com uma interface gráfica
			amigável, usando ícones, janelas e mouse. Sua acolhida foi extremamente entusiástica, grande parte disso
			devido as campanhas publicitárias em massa da Apple. O principal anúncio de seu lançamento foi durante o
			intervalo da <em>Super Bowl XVIII</em> (evento comparável com a importância da Copa do Mundo para o Brasil). Essa
			propaganda é conhecida como "1984", pois era baseada no livro <em>"Nineteen Eighty-Four" (Mil Novecentos e
			Oitenta e Quatro)</em> de <em>George Orwell</em>, e retrata um mundo no qual todos eram submetidos ao regime
			totalitário do "Big Brother" (Grande Irmão). Uma heroína representada por Anya Major destrói um telão no
			qual o Big Brother falava ao público. O intuito do comercial era relacionar a IBM ao "Big Brother" e a
			heroína à Apple.
		</p>
		<h2>Os "IBM-PC Compatíveis"</h2>
		<p>
			O mesmo grupo que criou o IBM-PC também definiu que o componente básico do computador, a <strong>BIOS</strong>, seria de
			fabricação exclusiva da IBM. Esse chip tem a finalidade de fornecer aos PCs uma interface de entrada e
			saída de dados. Como todos os outros componentes do computador eram fabricados por outras empresas, a IBM
			tinha nesses chips a sua maior fonte de renda e a única coisa que vinculava qualquer PC à IBM.
		</p>
		<p>
			Alguma empresas, dentre elas a <strong>Compaq</strong>, aplicaram a técnica de engenharia reversa no BIOS, clonaram-na e
			construíram computadores similares ao da IBM. Em novembro de 1982, a Compaq anuncia o <strong>Compaq Portable</strong>,
			primeiro PC que não usa a BIOS da IBM e mantém 100% de compatibilidade com o IBM PC.
		</p>
		<p>
			Esses computadores são conhecidos como <strong>IBM PC compatíveis</strong> e são os PCs que são vendidos nas lojas até
			hoje, apenas bem mais evoluídos do que os primeiros PCs. Isso levou a IBM a se tornar uma simples empresa
			que fabricava computadores pessoais e concorria como qualquer outra nesse mercado. A IBM praticamente
			abandonou o mercado de PCs e se dedicou ao mercado de servidores, na qual é imbatível até hoje.
		</p>
		<h2>Gerações de computadores</h2>
		<p>
			A arquitetura de um computador depende do seu projeto lógico, enquanto que a sua implementação depende da
			tecnologia disponível.
		</p>
		<p>
			As três primeiras gerações de computadores refletiam a evolução dos componentes básicos do computador
			(hardware) e um aprimoramento dos programas (software) existentes.
		</p>
		<p>
			Os computadores de <em>primeira geração</em> (19451959) usavam válvulas eletrônicas, quilômetros de fios, eram
			lentos, enormes e esquentavam muito.
		</p>
		<p>
			A <em>segunda geração</em> (19591964) substituiu as válvulas eletrônicas por transístores e os fios de ligação
			por circuitos impressos, o que tornou os computadores mais rápidos, menores e de custo mais baixo.
		</p>
		<p>
			A <em>terceira geração de computadores</em> (19641970) foi construída com circuitos integrados, proporcionando
			maior compactação, redução dos custos e velocidade de processamento da ordem de microssegundos. Tem
			início a utilização de avançados sistemas operacionais.
		</p>
		<p>
			A <em>quarta geração</em>, de 1970 até hoje, é caracterizada por um aperfeiçoamento da tecnologia já existente,
			proporcionando uma otimização da máquina para os problemas do usuário, maior grau de miniaturização,
			confiabilidade e maior velocidade, já da ordem de nanossegundos (bilionésima parte do segundo).
		</p>
		<p>
			O termo <em>quinta geração</em> foi criado pelos japoneses para descrever os potentes computadores "inteligentes"
			que queriam construir em meados da década de 1990. Posteriormente, o termo passou a envolver elementos de
			diversas áreas de pesquisa relacionadas à inteligência computadorizada: inteligência artificial, sistemas
			especialistas e linguagem natural.
		</p>
		<p>
			Mas o verdadeiro foco dessa ininterrupta quinta geração é a conectividade, o maciço esforço da indústria
			para permitir aos usuários conectarem seus computadores a outros computadores. O conceito de supervia da
			informação capturou a imaginação tanto de profissionais da computação como de usuários comuns.
		</p>
		<h2>Realizações para a sociedade</h2>
		<p>
			Apesar de sua pequena história enquanto uma disciplina acadêmica, a ciência da computação deu origem a
			diversas contribuições fundamentais para a ciência e para a sociedade. Esta ciência foi responsável pela
			definição formal de computação e computabilidade, e pela prova da existência de problemas insolúveis ou
			intratáveis computacionalmente. Também foi possível a construção e formalização do conceito de linguagem
			de computador, sobretudo linguagem de programação, uma ferramenta para a expressão precisa de informação
			metodológica flexível o suficiente para ser representada em diversos níveis de abstração.
		</p>
		<p>
			Para outros campos científicos e para a sociedade de forma geral, a ciência da computação forneceu
			suporte para a Revolução Digital, dando origem a Era da Informação. A computação científica é uma área da
			computação que permite o avanço de estudos como o mapeamento do genoma humano.
		</p>
	</body>
</html>